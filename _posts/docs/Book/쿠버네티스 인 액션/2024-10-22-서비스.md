---
title: 서비스에 대해 알아보기
tags: 인프라 도서
article_header:
type: cover
---

# 서비스 필요성

---

지금까지 배웠던 **파드**와 **레플리케이션 컨트롤러**는 독릭접인 작업을 수행할 수 있지만, 대개의 어플리케이션이 그러하듯
외부 요청에 응답하거나 같은 쿠버네티스 리소스 내부와 통신을 위해서 존재한다.

**MSA**환경에서의 **파드**는 결국 다른 **파드**와 통신을 하거나 외부 클라이언트의 요청과 통신을 한다.

쿠버네티스 환경이 아니라면, 서버 관리자가 **고정 IP**와 **포트**를 명시하고 그곳으로 접근하려고 하겠지만,
이전에 배웠던 **파드**의 **일시적**특성 때문에 **고정 IP**로 접근이 어렵다.

또한, **파드**는 노드에 할당받고 시작되기 전에 **IP**를 받기 때문에 클라이언트가 미리 **파드**의 **IP**를 알 수 없다.

추가로 **수평 스케일링**을 수행하는 **파드**는 같은 기능을 여러개의 파드에서 실행하지만, 클라이언트 입장에서 이러한 파드의 수와,
각각의 **IP**리스트를 가지고 있을 이유가 없다.

이러한 이유로 **쿠버네티스**는 **서비스**라는 리소스를 사용한다.

<br>
<br>

## 서비스란?

---

결국 중요하게 말했던 **파드**의 **유동적**특성으로 인해 접근이 어렵다는 문제를 **서비스**가 해결한다.

당연히 유동적인 파드와는 반대로 **정적인** **정보**를 가지며 그렇기 떄문에 서비스가 새롭게 생성되는게 아니라면
**단일 접점**만들어 **파드**를 묶어준다. 이러한 방식으로 이전에 말한 **서비스**의 필요성이 갖추어 진다.

<br>
<br>

## 서비스를 생성하는 법

---

서비스는 **한개이상의** 파드를 묶어주기 위해 존재한다. 이러한 파드들로 들어갈 정보를 먼저 받아 로드밸런싱을 지원한다.

하지만 **서비스**는 어떻게 자신이 로드밸런싱 해야하는 **파드**를 식별할까?

방법은 정말 간단한데 바로  **레이블 셀렉터**이다.

이전에 **레플리케이션 컨트롤러**가 관리해야 하는 **파드**를 식별하는 방법에 대해 배웠고 그것이 **레이블 셀렉터**를 이용하는 방법이라는 것을 경험했다.

서비스도 같은 방식으로 관리해야 할 **파드**를 식별한다.

<br><br>

### 명령어 방식 서비스 생성

---

서비스를 생성하는 가장 빠른 방법은 **kubectl expose** 명령어를 사용하는 것이다.
이는 파드 셀렉터를 사용해 서비스 리소스를 생성하고 모든 파드를 단일 iP 주소와 포트로 노출한다.

하지만, 이러한 명령어 방식보다 유지가능하고 명확한 **YAML**파일로 생성하는 것을 권장한다.

<br><br>

### YAML 방식 서비스 생성

---

````yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  ports:
    - port: 80
      targetPort: 8080
  selector:
    app: kubia
````

기본적인 **서비스**생성은 위 방식대로 진행하면 된다. 간략하게 설명하면 위 서비스는 포트 80의 연결을 허용하고,
그중 **app=kubia** 레이블 셀렉터와 일치하는 파드의 포트 8080으로 라우팅한다.

<br>
<br>

### 새 서비스 검사하기

---

방금전에 만든 서비스 명령어 (kubectl apply -f kubia-svc.yaml)을 통해서, 생성 된 서비스를 확인하기 위해서는 **kubectl get svc**를 수행하면 된다.

<br>

![](https://raw.githubusercontent.com/jickDo/picture/master/Book/K8s_in_action/cp5/get_svc.png)

사진에서 볼 수 있듯이 **클러스터 IP**가 할당된것을 볼 수 있다. **클러스터 IP**는 클러스터 내부에서만 액세스할 수 있다.

**서비스**의 기본목적은 파드 그룹을 클러스터의 다른 **파드**의 노출 시키는 것에 있기 때문에 현재는 **가상 IP**를 가지는 서비스만 생성한다.

하지만, 서비스도 결국은 외부 클라이언트와 통신하는 경우가 생기기 때문에, 나중에 그방법에 대해서 알아본다.


<br>
<br>

### 클러스터 내에서 서비스 테스트

---

클러스터내에서 서비스로 요청을 보내는 방법에 대해서 알아본다.

책에서 추천하는 가장 좋은 방법은, 서비스의 클러스터 **IP**로 요청을 보내고, 응답을 로그로 남기는 **파드**로 만드는 것이다.
그런 다음 파드의 로그를 검사해 서비스의 응답이 무엇인지 확인 가능하다.

이 방법은 좋지만 간단한 테스팅을 수행하기는 무겁기 때문에 넘어간다.

추가적으로, 쿠버네티스 노드로 ssh 접속하고 **curl**명령어를 날리는 방법과, **kubectl exec** 명령어로 기존 파드에서 **curl**명령을 실행할 수 있다.

<br>
<br>

### 실행중인 컨테이너에 원격으로 명령어 실행

----

**kubectl exec**명령어는 직접 파드에 접근하지 않아도 원격으로 명령어를 수행할 수 있게 한다.

실행중인 **파드**의 원격 curl을 위해 알아야 할 정보는 **파드**의 **이름** 그리고 서비스를 요청할 클러스터의 **IP** 가 필요하다.

````yaml
kubectl exec <파드이름> -- curl -s http://<클러스터 IP>
````

**여기서 더블대쉬(--)는 명령어가 끝났다는 것을 의미한다.**

만약에 위와같은 명령을 수핸하면 발생하는 플로우는

1. **kubectl exec** 명령을 통해 파드로 접근하다.
2. 접근한 파드에서 **curl**명령어를 수행한다.
3. **curl**명령은 **클러스터 IP**여기서는 서비스를 가르키기 때문에, 서비스로 요청이들어간다.
4. 서비스는 **curl**을 받고 자신이 관리하는 임의의 파드들중 하나로 요청을 전송한다.
5. 서비스에게 **curl**을 넘겨받는 **파드**는 원래의 요청지점으로 **curl** 응답을 보내준다.

이러한 과정을 거쳐서 **서비스**가 **파드**와 통신 후 정상적인 플로우로 흐른것을 확인할 수 있다.

<br>
<br>

### 서비스의 세션 어피니티 구성

---

들어가기 앞서 **세션 어피니티**를 설명하자면, 쿠버네티스에서 서비스 세션 어피니티는 클라이언트 요청이 특정 파드에 대한 지속적인 연결을 유지하도록 하는 기능이다.

**서비스 프록시**는 매번 같은 요청에 대해 다른 **파드**로 **로드 밸런싱**을 할 가능성이 존재한다. 당연히 그런 기능이 디폴트이다.

하지만 이러한 방식대신 매번 같은 파드로 리디렉션 하기위해 서비스의 **세션 어피티니** 속성을 추가할 수 있다.

<br>

````yaml
apiVersion: v1
kind: Service
spec:
  sessionAffinity: ClientIP
````

위 처럼 수행하면 서비스 프록시는 동일한 **클라이언트 IP**에 대해서 모든 요청을 동일한 파드로 전달한다.

쿠버네티스는 **None**과 **ClientIP**라는 두가지 유형의 서비스 세션 어피니티만 지원한다.

<br>
<br>

### 동일한 서비스에서 여러 개의 포트 노출

---

서비스는 단일 포트만 노출하지만, 여러 포트를 지원할 수 있다. 파드가 두 개의 포트를 수신한다면, 하나의 서비스를 사용해
포트 **80** **443**을 포트 **8080** 그리고 **8443** 으로 전달할 수 있다.

````yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  ports:
  - name: http
    port: 80
    targetPort: 8080
  - name: https
    port: 443
    targetPort: 8443
  selector:
    app: kubia
````

<br><br>

### 이름이 지정된 포트 사용

---

지금까지는 대상 포트를 실제값을 넣어줬지만, 그렇게 하게 되면 파드의 포트가 변하면 서비스의 스팩도 같이 변경해줘야 하는 번거로움이 있다.

**파드** 생성시 이름을 정의하면, 그 이름을 서비스에서 타겟 포트위치에 가져다가 쓸 수 있다.

````yaml
kind: Pod
spec:
  containers:
    - name: kubia
      ports:
        - name: http
          containerPort: 8080
        - name: https
          containerPort: 8443
````

<br>
<br>

````yaml
apiVersion: v1
kind: Service
spec:
  ports:
    - name: http
      port: 80
      targetPort: http
    - name: https
      port: 443
      targetPort: https
````

<br>
<br>

### 서비스의 검색

---

파드는 서비스의 **IP**와 **포트**로 엑세스 할 수 있어야 한다.
서비스는 정적이기 때문에 변하지 않는다. 하지만 파드가 생성 될 때 어떻게 **서비스**의 정보를 알 수 있는 걸까?

<br>

방법은 파드가 시작되기 전에 쿠버네티스는 그 시점에 존재하는 각 서비스를 가리키는 환경변수를 제공한다.

그렇기 떄문에 **서비스가 파드보다 나중에 생성되게 된다면 파드는 서비스의 정보를 담지 못한다.**

<br>
<br>

### DNS를 통한 서비스 검색

---

**kube-system** 네임스페이스에 파드를 조회하면, kube-dns가 조회된다. 같은 네임스페이스에 동일한 해당 서비스가 있다.

이름에서 알 수 있듯이, 이 파드는 **DNS** 서버를 실행하며 클러스터에서 실행 중인 다른 모든 파드는 자동으로 이를 사용하도록 구성된다.

따라서 파드에서 처리되는 모든 DNS 쿼리는 시스템에서 알고 있는 쿠버네티스의 자체 DNS 서버로 처리된다.

> 파드개 내부 DNS 서버를 사용할지 여부는 각 파드 스펙의 dnsPolicy 속성으로 구성할 수 있다.

각 서비스는 내부 DNS 서버에서 DNS 항목을 가져오고 서비스 이름을 알고 있는 클라이언트 파드는 환경변수 대신 FQDN(정규화된 도메인 이름)으로 엑세스 할 수 있다.

<br>

FQDN -> <서비스이름>.<네임스페이스>.<클러스터의 도메인 접속사>

<br>

ex) backend-database.default.svc.cluster.local

<br>

### 파드의 컨테이너 내에서 셸 실행

---

````yaml
kubectl exec -it <파드명> bash
````


<br>
<br>


> curl 과 비슷한 명령으로 ping이 있지만, 서비스에는 ping을 사용할 수없습니다. 기본적으로 클러스터 IP는 가상 IP기 때문이다.

<br>
<br>

# 클러스터 외부에 있는 서비스 연결

---

지금까지는 클러스터 내부에서 서비스를 이용해 파드가 통신하는 방법에 대해서 설명했다.

하지만, 어플리케이션을 운용하게 되면 결국 외부 클라이언트와 통신을 하게 될 필요가 있는데, 서비스가
이러한 역할을 수행하게 해준다.

<br>

## 서비스 엔드포인트

---

지금까지의 설명에서는 **서비스**는 파드에 직접 연결되어 통신하는 것처럼 보여젔다.

하지만, **서비스**는 **파드**에 직접 연결 되지 않는다.

그 사이에, **엔드포인트 리소스**가 있다.

먼저, 서비스를 조회하곘다.

![](https://raw.githubusercontent.com/jickDo/picture/master/Book/K8s_in_action/cp5/describe_svc.png)

현재는 서비스에 파드가 연결되어 있지 않아 **EndPoints**가 none으로 나오지만 원래 저곳에 파드의 **IP**목록이 나타나게 된다.

또한, 서비스의 파드 셀렉터가 이러한 엔드포인트를 만들어준다.

<br>
<br>


## 외부 서비스를 위한 별칭 생성

---

서비스의 엔드포인트를 수동으로 구성해 외부 서비스를 노출하는 대신 FQDN으로 외부 서비스를 참조할 수 있다.

<br>
<br>

### ExternalName 서비스 생성

---

지금 예시에서는 api.somecompany.com에 공개 API가 있다고 가정한다.

````yaml
apiVersion: v1
kind: Service
metadata:
  name: external-service
spec:
  type: ExternalName # 서비스 유형 설정
  externalName: someapi.somecompany.com # 실제 서비스의 정규화된 도메인 이름
  ports:
    - port: 80
````

위처럼 선언하게 되면 도메인 이름으로 접근이 가능하다. (external-service)으로 외부 서비스에 연결 할 수 있다.

위처럼 선언하게 되면 서비스를 사용하는 파드에서 실제 서비스 이름과 위치가 숨겨져 나중에 externalName 속성을 변경하거나 유형을 다시
ClusterIP로 변경하고 서비스 스펙을 만들어 서비스 스펙을 수정하면 나중에 다른 서비스를 가리킬 수 있다.

externalName 서비스는 DNS 레벨에서만 구현되기 때문에 간단한 CNAME DNS 레코드가 생성되고, 따라서 서비스에 연결하는 클라이언트는 서비스 프록시를 완전히
무시하고 외부 서비스에 직접 연결된다. 이러한 이유로 ExternalName 유형의 서비스는 ClusterIP를 얻지 못한다.

<br>
<br>
<br>

# 외부 클라이언트에 서비스 노출

---

지금까지는 클러스터 내부 단위에서 서비스가 요청을 받는 것을 알아보았다. 이제는 서비스를 외부에 노출해 클라이언트가 엑세스 할 수 있게 해야한다.

이러한 방법은 보통 세가지로 구현된다.

1. **노트 포트**로 서비스 유형설정
2. **로드밸런서** 서비스 유형설정
3. 단일 IP로 여러 서비스로 노출하는 **인그레스**

<br>
<br>


## 노드 포트 서비스

---

**노드포트 서비스**를 만들면 쿠버네티스는 모든 노드에 **특정 포트**를 할당하고 서비스를 구성하는 **파드**로 들어오는 연결을 전달한다.

<br>
<br>

````yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia-nodeport
spec:
  type: NodePort # 서비스 유형 노트포트로 설정
  ports:
    - port: 80 # 서비스 내부 클러스터 IP의 포트
      targetPort: 8080 # 서비스 대상 파드의 포트
      nodePort: 30123 # 각 클러스터 노드의 포트 30123으로 서비스에 엑세스할수있다.
  selector:
      app: kubia
````

유형을 노드포트로 설정하고 이서비스가 모든 클러스터 노드에 바인딩왜야 하는 노드포트로 지정한다.

![](https://raw.githubusercontent.com/jickDo/picture/master/Book/K8s_in_action/cp5/get_nodeport.png)

<br>

### BUT
노드 포트는 클러스터내 노드들에 노드포트를 모두 허용하여 어떤 노드든 접속이 가능하게 하지만, 클라이언트가 특정 노드에만 접속을 하여 장애가 생기는 것을
대비해서 요청을 분산시키는 **로드밸런서**형태를 사용하는것이 좋다.

<br>
<br>


## 로드밸런서 서비스

---

클라우드 제공자는 서비스에 **로드밸런서**를 선언해두면 자동으로 프로비저닝 하는 기능을 제공한다.
만약에 제공자가 **로드밸런서**를 제공하지 않는 환경이라고 할지라도 **로드밸런서**는 **노드포트**의 확장이기 때문에
노드포트 처럼 작동한다.

<br>
<br>

````yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia-loadbalancer
spec:
  type: LoadBalancer #로드밸런스 유형은 호스팅하는 인프라에서 제공한다.
  ports:
    - port: 80
      targetPort: 8080
  selector:
      app: kubia
````

위 파일을 apply 하게 되면 아래 사진과 같이 조회가 된다.

<br>

![](https://raw.githubusercontent.com/jickDo/picture/master/Book/K8s_in_action/cp5/get_svc_loadbalancer.png)

<br>
<br>

## 인그레스 리소스로 서비스 외부 노출

---

클러스터 외부의 클라이언트에 서비스를 노출하는 두 가지 방법을 살펴봤다. 마지막 방법은 **인그레스 리소스**이다

<br>

### 인그레스가 왜 필요할까?

---

로드밸런서는 자신의 공용 IP 주소를 가진 로드밸러서가 필요하지만, 인그레스는 한 **IP**를 가지고, 수십 개의 서비스에 접근하도록 허용한다.

아래 사진 처럼 인그레스가 들어오는 요청을 통해 서비스로 라우팅을 한다.

![](https://raw.githubusercontent.com/jickDo/picture/master/Book/K8s_in_action/cp5/ingress.png)

<br>
<br>

### 인그레스 컨트롤러

---

인그레스 오브젝트가 제공하는 기능을 살펴보기 전에 인그레스 리소스를 작동시키려면 클러스터에 **인그레스 컨트롤러**를 실행해야 한다.

<br>
<br>

### 인그레스 리소스 생성

---

````yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kubia
spec:
  rules:
    - hots: kubia.example.com
      http:
      paths:
        - path: /
          backend:
            serviceName: kubia-nodeport
            servicePort: 80
````

위 설정은 Kind를 **Ingress**로 설정하며, kubia.example.com 도메인 이름을 서비스에 매핑한다.
또한 모든 요청을 Kubia-nodeport 서비스의 포트 80으로 전달된다.

이제 /etc/hosts 위치에서 인그레스의 **IP**와 **kubia.example.com** 매핑해준다.

<br>
<br>

### 하나의 인그레스로 여러 서비스 노출

---

````yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kubia
spec:
  rules:
    - hots: kubia.example.com
      http:
      paths:
        - path: /kubia
          backend:
            serviceName: kubia
            servicePort: 80
        - path: /bar
          backend:
            serviceName: bar
            servicePort: 80
````

이 경우 요청은 URL의 경로에 따라 두 개의 다른 서비스로 전송된다. 따라서 클라이언트는 단일 IP 주소로 두 개의 서비스에 도달할 수 있다.

<br>

### 서로 다른 호스트로 서로 다른 서비스 매핑하기

---

````yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kubia
spec:
  rules:
    - hots: foo.example.com
      http:
      paths:
        - path: /
          backend:
            serviceName: foo
            servicePort: 80
    - hots: bar.example.com
      http:
      paths:
        - path: /
          backend:
            serviceName: bar
            servicePort: 80
````

위 처럼 호스트를 기준으로 서로 다른 서비스를 매핑 할 수 있다.

<br>
<br>

