---
title: 라이브니스 프로브는 어떻게 장애를 대응할까?
tags: 인프라 도서
article_header:
type: cover
---

## 레플리케이션

---

이제 **파드**가 쿠버네티스의 핵심 빌딩 단위이며, 중요하다는것을 알아보았다.

지금까지는 파드를 **수동**으로 생성, 감독, 관리를 하였지만 실제 환경에서는 **수동**관리가 힘들것이다.
컨테이너가 늘어남에 따라 그것을 관리, 감독 할 **무언가**를 필요로 했고, **쿠버네티스**가 탄생한것 처럼 말이다.

즉, 자동화가 필요하다. 이러한 자동화를 **레플리케이션 컨트롤러**나 이후에 배울 **디플로이먼트**가 수행한다.

이전에 배운 **파드**만으로도 장애가 발생 시 자동복구를 수행하는데 **왜?** 다른 리소스가 **파드**를 관리해야 할까?

이유는 **파드**보다 상위인 **노드**와 같은 리소스에서 장애가 발생할시 그것을 체크하고 장애대응을 할 친구가 필요하기 때문이다.

이번 장에서는 지속적으로 실행되는 파드를 실행하는 방법과, 한 가지 작업만 수행한 뒤 중지되는 파드를 배울것이다.

<br>
<br>

### 파드를 안정적으로 유지하기

---

쿠버네티스는 컨테이너 목록을 제공하면 해당 컨테이너를 클러스터 어딘가에서 계속 실행 되도록 하며, 파드를 생성하고,
워커노드를 지정하며, 해당 노드에서 파드의 컨테이너가 실행되도록 관리한다.

만약, 컨테이너가 죽거나, 파드 내부 모든 컨테이너가 죽게 된다면 해당 **노드**의 **kubelet**은 파드의
컨테이너를 실행하고 파드가 존재하는 한 컨테이너가 계속 실행되도록 한다.

컨테이너의 프로세스의 **crash(장애)** 가 발생한다면 **kubelet**이 컨테이너를 다시 시작한다.

하지만, 자바의 **OOME**같은 경우는 **crash**가 아니기 때문에 계속 작동할것이다. 애플리케이션이 죽어있지만,
**kubelet**은 이를 **장애**로 인지하지 못한다. 이러한 방법도 결국 처리할 수 있겠지만 잠깐뿐인 해결책이다.

만약에 **무한 루프**나 **교착 상태**가 발생한다면 이를 처리할 수 있을지 생각해보면 된다.

이러한 경우 내부적인 **장애**로 대응하기 힘들기 때문에, 우리는 이를 외부에서 **헬스체크**해야 한다.

<br>
<br>

### 라이브니스 프로브?

---

라이브니스 프로브는 컨테이너가 살아 있는지 확인할 수 있다.
정확히 **세가지** 메커니즘으로 확인 가능하다.

> HTTP GET 프로브

<br>

이는 지정한 IP주소로 HTTP GET요청을 보내어 헬스체크를 수행한다.
응답코드가 2xx or 3xx 인경우 성공으로 간주한다.
반대로 응답하지 않거나 에러 코드를 반환하면 실패로 간주후 다시 컨테이너를 실행한다.

<br>

> TCP 소켓 프로브

<br>

컨테이너의 지정된 포트에 TCP 연결을 시도한다. 연결에 성공하면 프로브가 성공한 것이고, 아니라면 다시시작한다.

<br>

> Exec 프로브

컨테이너 내의 임의의 명령을 실행하고 명령의 종료 상태 코드를 확인한다.
상태코드가 0이면 프로브가 성공한 것이다.

<br>
<br>

### HTTP 기반 라이브니스 프로스 생성

---

<br>

````yaml
apiVersion: v1
kind: pod
metadata:
    name: kubia-liveness
spec:
  containers:
    - image: luksa/kubia-unhealthy
      name: kubia
      livenessProbe:
        httpGet:
          path: /
          port: 8080
````

위와 같이 작성시 "/" 경로로 8080포트에 HTTP GET 요청을 보낸다.

<br>

> 크래시된 컨테이너의 애플리케이션 로그 얻기

> kubectl logs명령은 현재 로그를 출력한다. 이전 컨테이너의 장애를 확인하고 싶다면 **-previous**명령을 수행해야한다.

````yaml
kubectl logs <파드명> -previous
````

<br>
<br>

여기서 **exit code**를 주의해서 보아야 한다.
예를들어 **exit code**가 137이면 "128 + x"이다.
x가 실제로 컨테이너가 중지된 이유고 x가 9이니까 **SIGKILL** 인것이다.
여기서 나오는 x의 숫자는 리눅스 시그널을 참고해야 한다.

<br>

### 라이브니스 프로브의 추가 속성 설정

---

라이브니스 프로브는 **제한 시간**이나 **딜레이** **실패 임계 횟수**를 설정할 수 있다.

````yaml
livenessProbe:
  httpGet:
    path: /
    port: 8080
  initialDelaySeconds: 15
````

한가지 주의 해야할점을 위 설정에서 보여준다. 초기 지연을 설정하지 않으면 프로브는 컨테이너가 시작되자마자 프로브를 시작한다.
예를 들어 **스프링부트 서버**환경이면 서버가 작동하는데 시간이 걸릴것인데, 그 서버가 켜지기 전에 요청을 보내고
무조건 적인 실패를 응답 받는것이다.

<br>
<br>

### 효과적인 라이브니스 프로브

---

실제 운영환경에서 실행 중인 파드는 반드시 라이브니스 프로브를 정의해야 한다.
정의하지 않으면 쿠버네티스가 애플리케이션이 살아 있는지를 알 수 있는 방법이 없다.
프로세스가 실행되는 한 쿠버네티스는 컨테이너가 정상적이라고 가정할것이다.

<br>

> 몇가지 주의사항이 있다.

1. **라이브니스 프로브**는 **헬스 체크**용으로 사용하는 것이기 때문에 너무 많은 리소스를 잡아먹으면 안된다.
장애대응을 하기위해서 실제 **애플리케이션**의 성능을 많이 끌어다 쓰면 안된다.

<br>

2. **내부** 문제에 대해서만 체크해야 하는데, **프론트 웹서버**와 **백엔드 db** 영역이 있다고 했을 때,
웹서버가 디비연결에 실패했을 때 웹서버 외부적인 문제로 웹서버를 재실행 하면 안된다.
재실행한다고 해도 근본적인 문제가 **db** 에 있다면 해결되는 문제가 아니기 때문이다.

<br>

3. 재시도 루프를 구현하면 안된다. 프로브는 실패 임계값을 설정할수 있으며, 임계를 1로 설정해도 실패로 인지하기전에
여러번 시도한다.

<br>
<br>

### 라이브니스 프로브 요약

---

기본적인 파드의 장애는 **kubelet**을 통해서 수행한다. 그러나 **노드**에서 장애가 난경우는 **컨트롤 플레인**의 몫이다.
직접 생성한 **파드**는 노드 장애가 발생했을 때 장애복구를 할수없다.

