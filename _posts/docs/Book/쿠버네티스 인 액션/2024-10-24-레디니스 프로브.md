---
title: 레디니스 프로브와 헤드리스 서비스
tags: 인프라 도서
article_header:
type: cover
---

# 레디니스 프로브

---

레디니스 프로브를 설명하기 전에 예시 상황을 하나 만들겠다.

**서비스**는 **파드**의 **레이블 셀렉터**와 일치할 경우 파드가 서비스의 엔드포인트를 포함한다는 것을 배웠다.

적절한 레이블을 가진 새 파드가 만들어지자마자 서비스의 일부가 돼 요청이 파드로 전달되기 시작한다.

즉 파드가 생성됨과 동시에 서비스와 연결되었지만, 실제 **파드의 준비**가 완료되지 않은 상태가 생긴다면 어떨까?

당연히 요청은 실패할것이고 이는 사용자 경험에 영향을 미친다.


<br>
<br>

## 레디니스 프로브란?

---

이전에 배운 **라이브 니스 프로브**와 이름이 유사한 **레디니스 프로브**는 기능도 유사하다고 할 수 있다.

다만, **라이브니스 프로브**는 **장애 대응**에 목적이 맞추어져 있어 문제가 생겼을때 이를 대응하기 위해 존재하는 반면에

**레디니스 프로브**는 **특정 파드**가 준비가 되었는지 체크하는 용도로 사용한다.

<br>
<br>

## 레디니스 프로브 유형

---

레디니스 프로브는 세가지 유형이 있다.

1. 프로세스를 실행하는 Exec 프로브는 컨테이너의 상태를 프로세스의 종료 상태 코드로 결정한다.
2. HTTP GET 프로브는 HTTP GET 요청을 컨테이너로 보내고 응답의 HTTP 상태 코드를 보고 컨테이너가 준비됐는지 여부를 결정한다.
3. TCP 소켓 프로브는 컨테이너의 지정된  포트로 TCP 연결을 연다. 소켓이 연결되면 컨테이너가 준비된 것으로 간주한다.


<br>
<br>

### 레디니스 프로브의 동작

---

컨테이너가 시작될 떄 쿠버네티스는 첫 번째 레디니스 점걸을 **initailDelaySeconds**을 이용해서 구성할 수 있다. 그런 다음 주기적으로 프로브를 호출하고 레디니스 프로브의 결과에 따라
작동하게 할 수 있다. 파드가 준비되지 않았다고 하면 서비스에서 제거하고, 준비가 완료되면 서비스에 다시 추가한다.

**라이브니스 프로브**는 점검이 실패한다면 컨테이너를 종료하거나 다시 시작하는 반면, **라이브니스 프로브**는 그렇지 않다.

<br>
<br>

### 레디니스 프로브가 중요한 이유

---

어플리케이션 서버를 실행하는 파드가 다른 데이터 베이스 같은 파드에 의존한다고 가정하면, 프론트엔드 파드 중 문제가 발생해
데이터 베이스 파드에 연결할 수 없는 경우, 해당 시점에 바로 준비가 되어있지 않다고 알리는 것이 좋다.

그렇게 해야만 문제가 발생하지 않은 파드와 연결을 이어주고 사용자입장에서는 파드에 문제가 발생했다는 사실조차
인지하지 못하게 할 수 있다.

<br>
<br>

### 레디니스 프로브 추가

---

레디니스 프로브는 파드의 템플릿에 추가한다.

````yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
        - name: kubia
          image: luksa/kubia
          ports:
            - containerPort: 8080
          readinessProbe:
            exec:
              command:
                - ls
                - /var/ready #파드의 각 컨테이너에 레디니스 프로브가 정의될 수 있다.
                            # var/ready가 있다면 종료코드 0을 반환하고 파드가 생성되어 서비스 엔드포인트에 추가될것이다.
                            # 가상의 스토리이기 때문에 실제로 이렇게 구현하면 안된다.
````

<br>
<br>

위 선언 대로면 **레디니스 프로브**는 매번 파드의 /var/ready 명령어를 주기적으로 수행하고 존재하면 종료코드를 반환한다.

이렇게 이상한 조건을 거는 이유는 문제의 파일을 생성하거나 제거해 그 결과를 바로 전환가능하기 때문이다.

<br>
<br>

### 실제 환경에서 레디니스 프로브가 수행해야 하는 기능

---

실제환경에서는 위경우처럼 선언하면 안되고, 프로브가 클라이언트 요청을 수신할 수 있는지 여부에 따라 성공 또는 실패를 반환하게 만들어야 한다.

<br>
<br>

### 레디니스 프로브를 항상 정의하라

---

책에서는 **레디니스 프로브**에서 강조해야 할 두가지 사항을 말한다. 먼저 파드에 **레디니스 프로브**를 추가하지 않으면 파드가 시작하는 즉시 서비스 엔드포인트가 된다.
이러한 상황을 파드가 준비되어있지 않아도 클라이언트가 접근이 가능한 문제를 범할 수 있다. 그렇기 때문에 항상**레디니스 프로브**를 선언해야 한다.

<br>
<br>

### 레디니스 프로브에 파드의 종료 코드를 포함하지 마라

---

파드가 종료할 때, 실행되는 어플리케이션은 종료 신호를 받자마자 연결 수락을 중단한다.

그렇기 때문에 종료 절차가 시작되는 즉시 레디니스 프로브가 실행하도록 만들어 파드가 모든 서비스에서 확실하게 제거돼야 한다고 생각할 수있지만

파드가 종료되는 즉시 모든 서비스에서 파드를 제거하기 때문에 종료 코드를 포함하면 안된다.

<br>
<br>
<br>
<br>

# 헤드리스 서비스로 개별 파드 찾기

---

지금까지 서비스의 파드에 클라이언트의 연결을 허용하려고 서비스가 안정적인 IP 주소를 제공하는 방법을 살펴봤다.

서비스의 연결은 임의의 파드로 전달된다.

그러나 클라이언트가 모든 파드에 연결해야 하는 경우 어떻게 해야 할까?

클라이언트는 모든 파드에 연결하려면 각 파드의 **IP**를 알아야 한다.

한 가지 옵션은 클라이언트가 쿠버네티스 API 서버를 호출해 파드와 IP 주소 목록을 가져오는것이다.

이러한 방법은 에플리케이션을 쿠버네티스와 유관하게 유지하게 때문에 추천안한다.

쿠버네티스는 클라이언트가 DNS 조회로 파드 IP 를 찾을 수 있도록 한다. 보통 조회를 수행하면 **클러스터 IP**를 반환한다.

그러나 쿠버네티스 서비스에 **클러스터 IP**가 필요하지 않다면, **클러스터 IP**설정을 **NONE**으로 한다. 그렇게 되면 DNS서버는 **파드IP**를 반환한다.

<br><br>

## 헤드리스 서비스 생성

---

서비스 스펙의 **ClusterIP** 필드를 **None**으로 설정하면 쿠버네티스는 클라이언트가 서비스의 파드에 연결할 수 있는
**클러스터 IP**를 할당하지 않기 때문에 서비스가 헤드리스 상태가 된다.

````yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia-headless
spec:
  clusterIP: None # 헤드리스 선언법
  ports:
    - port: 80
      targetPort: 8080
  selector:
    app: kubia
````

위 처럼 선언하게 되면, **클러스터 IP**가 없고, 엔드포인트에 파드 셀렉터와 일치하는 파드가 포함된다.




